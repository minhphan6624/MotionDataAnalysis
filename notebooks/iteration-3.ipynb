{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "sys.path.append('../')  # Go up one directory\n",
    "\n",
    "from src.data_splitting import split_data, get_split_shapes\n",
    "from src.model_training import get_models, train_models_for_task\n",
    "from src.model_evaluation import print_results, print_result_for_task, summarize_results, plot_confusion_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Import and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (7956, 135)\n",
      "\n",
      "main_activity splits:\n",
      "X_train shape: (6364, 132)\n",
      "X_test shape: (1592, 132)\n",
      "y_train shape: (6364,)\n",
      "y_test shape: (1592,)\n",
      "\n",
      "label splits:\n",
      "X_train shape: (6364, 132)\n",
      "X_test shape: (1592, 132)\n",
      "y_train shape: (6364,)\n",
      "y_test shape: (1592,)\n",
      "\n",
      "sharpness splits:\n",
      "X_train shape: (6364, 132)\n",
      "X_test shape: (1592, 132)\n",
      "y_train shape: (6364,)\n",
      "y_test shape: (1592,)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Read in the cleaned data\n",
    "cleaned_df = pd.read_csv('..\\\\data\\\\final_data\\\\cleaned_train_data.csv')\n",
    "print(f\"Dataset shape: {cleaned_df.shape}\")\n",
    "\n",
    "# Shuffle the data\n",
    "cleaned_df = cleaned_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "splits = split_data(cleaned_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(get_split_shapes(splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Feature engineering - Magnitude features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magnitude features added\n",
      "Number of magnitude features: 44\n",
      "(7956, 179)\n",
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "joints = ['L5', 'L3', 'T12', 'T8', 'Neck', 'Head',\n",
    "          'Right Shoulder', 'Right Upper Arm', 'Right Forearm', 'Right Hand',\n",
    "            'Left Shoulder', 'Left Upper Arm', 'Left Forearm', 'Left Hand', \n",
    "            'Right Upper Leg', 'Right Lower Leg', 'Right Foot', 'Right Toe',\n",
    "            'Left Upper Leg', 'Left Lower Leg', 'Left Foot', 'Left Toe', ]\n",
    "\n",
    "for joint in joints:\n",
    "\n",
    "    vel_cols = [f'{joint} x_Vel', f'{joint} y_Vel', f'{joint} z_Vel']\n",
    "    acc_cols = [f'{joint} x_Acc', f'{joint} y_Acc', f'{joint} z_Acc']\n",
    "\n",
    "\n",
    "    if all(col in cleaned_df.columns for col in vel_cols):\n",
    "        cleaned_df[f'{joint}_Vel_Magnitude'] = np.sqrt(\n",
    "            cleaned_df[vel_cols[0]]**2 + \n",
    "            cleaned_df[vel_cols[1]]**2 + \n",
    "            cleaned_df[vel_cols[2]]**2\n",
    "        )\n",
    "    \n",
    "    # Acceleration magnitude\n",
    "    if all(col in cleaned_df.columns for col in acc_cols):\n",
    "        cleaned_df[f'{joint}_Acc_Magnitude'] = np.sqrt(\n",
    "            cleaned_df[acc_cols[0]]**2 + \n",
    "            cleaned_df[acc_cols[1]]**2 + \n",
    "            cleaned_df[acc_cols[2]]**2\n",
    "        )\n",
    "\n",
    "print(\"Magnitude features added\")\n",
    "magnitude_cols = [col for col in cleaned_df.columns if 'Magnitude' in col]\n",
    "print(f\"Number of magnitude features: {len(magnitude_cols)}\")\n",
    "print(cleaned_df.shape)\n",
    "\n",
    "# Print the columns with misisng values > 0\n",
    "print(cleaned_df.columns[cleaned_df.isna().sum() > 0])\n",
    "\n",
    "# Fill the NaN values with the mean of the column\n",
    "# cleaned_df.fillna(cleaned_df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Feature engineering - Roll features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling features added\n",
      "Number of rolling features: 44\n"
     ]
    }
   ],
   "source": [
    "# Define window sizes for rolling calculations\n",
    "window_size = 5  # Single window size for simplicity\n",
    "\n",
    "# Calculate rolling mean for magnitude columns\n",
    "for col in magnitude_cols:\n",
    "    # Group by label to avoid mixing statistics across different activities\n",
    "    grouped = cleaned_df.groupby('Label')[col]\n",
    "    \n",
    "    # Calculate rolling mean only\n",
    "    cleaned_df[f'{col}_RollingMean_{window_size}'] = grouped.transform(\n",
    "        lambda x: x.rolling(window=window_size, min_periods=1).mean())\n",
    "\n",
    "print(\"Rolling features added\")\n",
    "rolling_cols = [col for col in cleaned_df.columns if 'Rolling' in col]\n",
    "print(f\"Number of rolling features: {len(rolling_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n",
      "(7956, 223)\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_df.columns[cleaned_df.isna().sum() > 0])\n",
    "print(cleaned_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(features_df, title, target_name, top_n=20):\n",
    "    \"\"\"Visualize feature importance\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='importance', y='feature', data=features_df.head(top_n))\n",
    "    plt.title(f'Top {top_n} Important Features for {title}\\nTarget: {target_name}')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 220\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for feature selection\n",
    "feature_cols = [col for col in cleaned_df.columns \n",
    "                if col not in ['Main_Activity', 'Label', 'Knife_Sharpness_Category']]\n",
    "X = cleaned_df[feature_cols]\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "print (f\"Number of features: {len(feature_cols)}\")\n",
    "\n",
    "# Dictionary to store selected features for each target\n",
    "selected_features = {}\n",
    "\n",
    "# Define targets and how many features to select for each\n",
    "targets = {\n",
    "    'main_activity': ('Main_Activity', 50),  # (target_column, n_features)\n",
    "    'label': ('Label', 70),\n",
    "    'sharpness': ('Knife_Sharpness_Category', 50)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Perform feature selection for each target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature selection for main_activity\n",
      "\n",
      "Number of selected features for main_activity: 27\n",
      "\n",
      "Top 10 selected features:\n",
      "['T12_Vel_Magnitude_RollingMean_5', 'Right Upper Arm_Acc_Magnitude_RollingMean_5', 'L3_Acc_Magnitude_RollingMean_5', 'L5 y_Vel', 'T12_Vel_Magnitude', 'Left Forearm_Acc_Magnitude_RollingMean_5', 'Right Lower Leg_Acc_Magnitude', 'Neck_Vel_Magnitude', 'T8_Acc_Magnitude_RollingMean_5', 'L3 z_Vel']\n",
      "\n",
      "Feature type breakdown:\n",
      "Original features: 9\n",
      "Magnitude features: 18\n",
      "Rolling features: 11\n",
      "\n",
      "Feature selection for label\n",
      "\n",
      "Number of selected features for label: 58\n",
      "\n",
      "Top 10 selected features:\n",
      "['Left Lower Leg_Acc_Magnitude_RollingMean_5', 'Left Lower Leg_Vel_Magnitude_RollingMean_5', 'T12_Vel_Magnitude_RollingMean_5', 'Right Upper Arm_Acc_Magnitude_RollingMean_5', 'L3_Acc_Magnitude_RollingMean_5', 'Left Toe_Vel_Magnitude_RollingMean_5', 'Right Lower Leg_Acc_Magnitude_RollingMean_5', 'Left Forearm_Acc_Magnitude_RollingMean_5', 'Right Upper Leg_Vel_Magnitude_RollingMean_5', 'Left Upper Leg_Acc_Magnitude']\n",
      "\n",
      "Feature type breakdown:\n",
      "Original features: 0\n",
      "Magnitude features: 58\n",
      "Rolling features: 44\n",
      "\n",
      "Feature selection for sharpness\n",
      "\n",
      "Number of selected features for sharpness: 32\n",
      "\n",
      "Top 10 selected features:\n",
      "['L5 x_Vel', 'L5 y_Vel', 'T12_Vel_Magnitude', 'Left Upper Leg_Acc_Magnitude', 'Right Upper Arm_Acc_Magnitude', 'Neck_Vel_Magnitude', 'Right Upper Leg_Acc_Magnitude', 'L3 z_Vel', 'Right Upper Leg z_Acc', 'Neck x_Vel']\n",
      "\n",
      "Feature type breakdown:\n",
      "Original features: 15\n",
      "Magnitude features: 17\n",
      "Rolling features: 1\n"
     ]
    }
   ],
   "source": [
    "for target_key, (target_col, n_features) in targets.items():\n",
    "    print(f\"\\nFeature selection for {target_key}\")\n",
    "    y = cleaned_df[target_col]\n",
    "    \n",
    "    # 1. Random Forest Importance\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    rf_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot Random Forest importance\n",
    "    # plot_feature_importance(rf_importance, f\"{target_key} (Random Forest)\")\n",
    "    \n",
    "    # 2. ANOVA F-scores\n",
    "    f_selector = SelectKBest(score_func=f_classif, k='all')\n",
    "    f_selector.fit(X, y)\n",
    "    f_scores = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': f_selector.scores_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot ANOVA F-scores\n",
    "    # plot_feature_importance(f_scores, f\"{target_key} (ANOVA F-scores)\")\n",
    "    \n",
    "    # Select features that are important in both methods\n",
    "    top_rf = set(rf_importance.head(n_features)['feature'])\n",
    "    top_f = set(f_scores.head(n_features)['feature'])\n",
    "    \n",
    "    # Get features that appear in both methods\n",
    "    common_features = list(top_rf.intersection(top_f))\n",
    "    selected_features[target_key] = common_features\n",
    "    \n",
    "    print(f\"\\nNumber of selected features for {target_key}: {len(common_features)}\")\n",
    "    print(\"\\nTop 10 selected features:\")\n",
    "    print(common_features[:10])\n",
    "    \n",
    "    # Analysis of selected feature types\n",
    "    magnitude_selected = len([f for f in common_features if 'Magnitude' in f])\n",
    "    rolling_selected = len([f for f in common_features if 'Rolling' in f])\n",
    "    original_selected = len([f for f in common_features \n",
    "                           if 'Magnitude' not in f and 'Rolling' not in f])\n",
    "    \n",
    "    print(f\"\\nFeature type breakdown:\")\n",
    "    print(f\"Original features: {original_selected}\")\n",
    "    print(f\"Magnitude features: {magnitude_selected}\")\n",
    "    print(f\"Rolling features: {rolling_selected}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save selected features of the label and sharpness target to a JSON file\n",
    "import json\n",
    "\n",
    "with open('selected_features.json', 'w') as f:\n",
    "    json.dump(selected_features, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating final datasets with selected features...\n",
      "\n",
      "main_activity dataset shape: (7956, 28)\n",
      "\n",
      "label dataset shape: (7956, 59)\n",
      "\n",
      "sharpness dataset shape: (7956, 33)\n",
      "\n",
      "Feature selection complete!\n",
      "\n",
      "Summary of selected features:\n",
      "\n",
      "main_activity:\n",
      "Number of features selected: 27\n",
      "\n",
      "label:\n",
      "Number of features selected: 58\n",
      "\n",
      "sharpness:\n",
      "Number of features selected: 32\n"
     ]
    }
   ],
   "source": [
    "# Create final datasets with selected features\n",
    "print(\"\\nCreating final datasets with selected features...\")\n",
    "selected_dfs = {}\n",
    "for target_key, features in selected_features.items():\n",
    "    if target_key == 'main_activity':\n",
    "        target_col = 'Main_Activity'\n",
    "    elif target_key == 'label':\n",
    "        target_col = 'Label'\n",
    "    else:  # sharpness\n",
    "        target_col = 'Knife_Sharpness_Category'\n",
    "    \n",
    "    # Create new dataframe with only selected features and target\n",
    "    selected_df = pd.concat([\n",
    "        cleaned_df[features],  # Selected features\n",
    "        cleaned_df[target_col]  # Target variable\n",
    "    ], axis=1)\n",
    "    \n",
    "    selected_dfs[target_key] = selected_df\n",
    "    print(f\"\\n{target_key} dataset shape: {selected_df.shape}\")\n",
    "\n",
    "# Save the selected features for reference\n",
    "feature_selection_summary = {\n",
    "    target: {\n",
    "        'selected_features': features,\n",
    "        'n_features': len(features)\n",
    "    }\n",
    "    for target, features in selected_features.items()\n",
    "}\n",
    "\n",
    "print(\"\\nFeature selection complete!\")\n",
    "print(\"\\nSummary of selected features:\")\n",
    "for target, info in feature_selection_summary.items():\n",
    "    print(f\"\\n{target}:\")\n",
    "    print(f\"Number of features selected: {info['n_features']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Split data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data splits after feature selection:\n",
      "\n",
      "main_activity splits:\n",
      "X_train shape: (6364, 27)\n",
      "X_test shape: (1592, 27)\n",
      "y_train shape: (6364,)\n",
      "y_test shape: (1592,)\n",
      "\n",
      "label splits:\n",
      "X_train shape: (6364, 58)\n",
      "X_test shape: (1592, 58)\n",
      "y_train shape: (6364,)\n",
      "y_test shape: (1592,)\n",
      "\n",
      "sharpness splits:\n",
      "X_train shape: (6364, 32)\n",
      "X_test shape: (1592, 32)\n",
      "y_train shape: (6364,)\n",
      "y_test shape: (1592,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "splits = {}\n",
    "for target_key, df in selected_dfs.items():\n",
    "    # Get features and target\n",
    "    X = df.drop(columns=['Main_Activity' if target_key == 'main_activity' \n",
    "                        else 'Label' if target_key == 'label' \n",
    "                        else 'Knife_Sharpness_Category'])\n",
    "    y = df['Main_Activity' if target_key == 'main_activity' \n",
    "           else 'Label' if target_key == 'label' \n",
    "           else 'Knife_Sharpness_Category']\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    splits[target_key] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"\\nData splits after feature selection:\")\n",
    "for target, (X_train, X_test, y_train, y_test) in splits.items():\n",
    "    print(f\"\\n{target} splits:\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Class balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: Main_activity\n",
      "Original class distribution: {0: np.int64(3350), 1: np.int64(3014)}\n",
      "Resampled class distribution: {1: np.int64(2970), 0: np.int64(2970)}\n",
      "Original shape: (6364, 27), Resampled shape: (5940, 27)\n",
      "\n",
      "Target: Label\n",
      "Original class distribution: {4: np.int64(3187), 0: np.int64(765), 5: np.int64(733), 2: np.int64(598), 8: np.int64(433), 3: np.int64(290), 1: np.int64(170), 7: np.int64(103), 6: np.int64(85)}\n",
      "Resampled class distribution: {7: np.int64(3187), 6: np.int64(3187), 1: np.int64(3187), 8: np.int64(3187), 2: np.int64(3187), 3: np.int64(3186), 0: np.int64(3186), 5: np.int64(3185), 4: np.int64(3183)}\n",
      "Original shape: (6364, 58), Resampled shape: (28675, 58)\n",
      "\n",
      "Target: Sharpness\n",
      "Original class distribution: {2: np.int64(2802), 1: np.int64(2481), 0: np.int64(1081)}\n",
      "Resampled class distribution: {0: np.int64(2773), 1: np.int64(2570), 2: np.int64(2563)}\n",
      "Original shape: (6364, 32), Resampled shape: (7906, 32)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Initialize SMOTETomek\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "\n",
    "# Loop through each target variable to apply SMOTE-Tomek\n",
    "balanced_data = {}\n",
    "for target in ['main_activity', 'label', 'sharpness']:\n",
    "\n",
    "    # Get the data splits\n",
    "    X_train, X_test, y_train, y_test = splits[target]\n",
    "    print(f\"\\nTarget: {target.capitalize()}\")\n",
    "    print(f\"Original class distribution: {dict(pd.Series(y_train).value_counts())}\")\n",
    "\n",
    "    # Apply SMOTE-Tomek\n",
    "    X_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "    balanced_data[target] = (X_resampled, X_test, y_resampled, y_test)\n",
    "\n",
    "    print(f\"Resampled class distribution: {dict(pd.Series(y_resampled).value_counts())}\")\n",
    "    print(f\"Original shape: {X_train.shape}, Resampled shape: {X_resampled.shape}\")\n",
    "\n",
    "\n",
    "    # print(f\"\\nTarget: {target.capitalize()}\")\n",
    "    # print(f\"Original class distribution: {dict(pd.Series(y_train).value_counts())}\")\n",
    "    # print(f\"Resampled class distribution: {dict(pd.Series(y_resampled).value_counts())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization complete for Main_activity\n",
      "Training data shape: (5940, 27)\n",
      "Test data shape: (1592, 27)\n",
      "Normalization complete for Label\n",
      "Training data shape: (28675, 58)\n",
      "Test data shape: (1592, 58)\n",
      "Normalization complete for Sharpness\n",
      "Training data shape: (7906, 32)\n",
      "Test data shape: (1592, 32)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "normalized_data = {}\n",
    "\n",
    "# Loop through balanced data and normalize\n",
    "for target, (X_resampled, X_test, y_resampled, y_test) in balanced_data.items():\n",
    "\n",
    "    # Fit on training data, transform both training and test\n",
    "    X_normalized = scaler.fit_transform(X_resampled)\n",
    "    X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "    normalized_data[target] = (X_normalized, X_test_normalized, y_resampled, y_test)\n",
    "    \n",
    "    print(f\"Normalization complete for {target.capitalize()}\")\n",
    "    print(f\"Training data shape: {X_normalized.shape}\")\n",
    "    print(f\"Test data shape: {X_test_normalized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "main_activity splits:\n",
      "X_train shape: (5940, 27)\n",
      "X_test shape: (1592, 27)\n",
      "y_train shape: (5940,)\n",
      "y_test shape: (1592,)\n",
      "\n",
      "label splits:\n",
      "X_train shape: (28675, 58)\n",
      "X_test shape: (1592, 58)\n",
      "y_train shape: (28675,)\n",
      "y_test shape: (1592,)\n",
      "\n",
      "sharpness splits:\n",
      "X_train shape: (7906, 32)\n",
      "X_test shape: (1592, 32)\n",
      "y_train shape: (7906,)\n",
      "y_test shape: (1592,)\n"
     ]
    }
   ],
   "source": [
    "# print the targets from normalized data\n",
    "for target, (X_train, X_test, y_train, y_test) in normalized_data.items():\n",
    "    print(f\"\\n{target} splits:\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "\n",
      "Training Models for Target: Main_activity\n",
      "\n",
      "Training Logistic Regression...\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Training XGBoost...\n",
      "----------------------------------------\n",
      "\n",
      "Training Models for Target: Label\n",
      "\n",
      "Training Decision Tree...\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Training XGBoost...\n",
      "----------------------------------------\n",
      "\n",
      "Training Models for Target: Sharpness\n",
      "\n",
      "Training Decision Tree...\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Training XGBoost...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Dictionary to store results for each target\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "# Train and evaluate models\n",
    "for target, (X_train, X_test, y_train, y_test) in normalized_data.items():\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"\\nTraining Models for Target: {target.capitalize()}\")\n",
    "   \n",
    "    if target == 'main_activity':\n",
    "        models = get_models(task_type='binary')\n",
    "    else:\n",
    "        models = get_models(task_type=\"multiclass\")  \n",
    "\n",
    "    target_results = {}\n",
    "    target_trained_models = {}\n",
    "    for model_name, model in models.items():\n",
    "        \n",
    "        # Train model\n",
    "        print(f\"\\nTraining {model_name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        target_trained_models[model_name] = model\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        target_results[model_name] = {\n",
    "            'model': model,\n",
    "            'accuracy': accuracy,\n",
    "            'report': class_report,\n",
    "            'confusion_matrix': conf_matrix,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "    \n",
    "    results[target] = target_results\n",
    "    trained_models[target] = target_trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for main_activity:\n",
      "--------------------------------------------------\n",
      "\n",
      "Logistic Regression:\n",
      "Accuracy: 0.7519\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.76      0.76       838\n",
      "           1       0.74      0.74      0.74       754\n",
      "\n",
      "    accuracy                           0.75      1592\n",
      "   macro avg       0.75      0.75      0.75      1592\n",
      "weighted avg       0.75      0.75      0.75      1592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[639 199]\n",
      " [196 558]]\n",
      "\n",
      "Random Forest:\n",
      "Accuracy: 0.7814\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.79       838\n",
      "           1       0.77      0.77      0.77       754\n",
      "\n",
      "    accuracy                           0.78      1592\n",
      "   macro avg       0.78      0.78      0.78      1592\n",
      "weighted avg       0.78      0.78      0.78      1592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[660 178]\n",
      " [170 584]]\n",
      "\n",
      "XGBoost:\n",
      "Accuracy: 0.7927\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.80       838\n",
      "           1       0.78      0.78      0.78       754\n",
      "\n",
      "    accuracy                           0.79      1592\n",
      "   macro avg       0.79      0.79      0.79      1592\n",
      "weighted avg       0.79      0.79      0.79      1592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[677 161]\n",
      " [169 585]]\n",
      "\n",
      "Results for label:\n",
      "--------------------------------------------------\n",
      "\n",
      "Decision Tree:\n",
      "Accuracy: 0.6683\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.87       191\n",
      "           1       0.83      0.79      0.81        43\n",
      "           2       0.75      0.82      0.78       150\n",
      "           3       0.35      0.49      0.41        72\n",
      "           4       0.85      0.67      0.75       797\n",
      "           5       0.32      0.43      0.37       184\n",
      "           6       0.18      0.29      0.22        21\n",
      "           7       0.32      0.65      0.43        26\n",
      "           8       0.54      0.65      0.59       108\n",
      "\n",
      "    accuracy                           0.67      1592\n",
      "   macro avg       0.56      0.63      0.58      1592\n",
      "weighted avg       0.72      0.67      0.68      1592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[166   1  15   0   3   4   0   0   2]\n",
      " [  1  34   0   4   4   0   0   0   0]\n",
      " [ 11   0 123   1   7   6   0   0   2]\n",
      " [  0   1   1  35   8   5   3   5  14]\n",
      " [ 10   4  19  25 533 143  15  21  27]\n",
      " [  4   1   6  10  56  80   8   5  14]\n",
      " [  0   0   0   4   4   4   6   3   0]\n",
      " [  0   0   0   4   3   0   1  17   1]\n",
      " [  0   0   0  16  12   8   0   2  70]]\n",
      "\n",
      "Random Forest:\n",
      "Accuracy: 0.7971\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94       191\n",
      "           1       0.91      1.00      0.96        43\n",
      "           2       0.83      0.91      0.87       150\n",
      "           3       0.61      0.62      0.62        72\n",
      "           4       0.89      0.81      0.85       797\n",
      "           5       0.48      0.54      0.51       184\n",
      "           6       0.43      0.62      0.51        21\n",
      "           7       0.53      0.81      0.64        26\n",
      "           8       0.73      0.81      0.77       108\n",
      "\n",
      "    accuracy                           0.80      1592\n",
      "   macro avg       0.71      0.79      0.74      1592\n",
      "weighted avg       0.81      0.80      0.80      1592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[179   0   8   1   2   1   0   0   0]\n",
      " [  0  43   0   0   0   0   0   0   0]\n",
      " [  5   0 137   0   4   4   0   0   0]\n",
      " [  2   1   0  45   5   2   2   2  13]\n",
      " [  1   2  14   8 643  94   9  11  15]\n",
      " [  1   0   6   7  57 100   5   4   4]\n",
      " [  0   1   0   1   1   2  13   2   1]\n",
      " [  0   0   0   2   2   0   1  21   0]\n",
      " [  0   0   0  10   5   5   0   0  88]]\n",
      "\n",
      "XGBoost:\n",
      "Accuracy: 0.8160\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       191\n",
      "           1       0.89      0.98      0.93        43\n",
      "           2       0.87      0.92      0.90       150\n",
      "           3       0.69      0.67      0.68        72\n",
      "           4       0.92      0.83      0.87       797\n",
      "           5       0.52      0.63      0.57       184\n",
      "           6       0.34      0.52      0.42        21\n",
      "           7       0.50      0.73      0.59        26\n",
      "           8       0.72      0.81      0.77       108\n",
      "\n",
      "    accuracy                           0.82      1592\n",
      "   macro avg       0.71      0.78      0.74      1592\n",
      "weighted avg       0.84      0.82      0.82      1592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[179   1   7   1   0   2   0   0   1]\n",
      " [  1  42   0   0   0   0   0   0   0]\n",
      " [  5   0 138   0   1   5   0   0   1]\n",
      " [  1   1   0  48   6   1   2   1  12]\n",
      " [  0   2   8   4 658  93   9  10  13]\n",
      " [  0   0   5   4  42 116   7   4   6]\n",
      " [  0   1   0   1   1   3  11   3   1]\n",
      " [  0   0   0   1   3   1   2  19   0]\n",
      " [  0   0   0  11   3   4   1   1  88]]\n",
      "\n",
      "Results for sharpness:\n",
      "--------------------------------------------------\n",
      "\n",
      "Decision Tree:\n",
      "Accuracy: 0.4956\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.41      0.34       270\n",
      "           1       0.51      0.50      0.51       621\n",
      "           2       0.59      0.53      0.56       701\n",
      "\n",
      "    accuracy                           0.50      1592\n",
      "   macro avg       0.47      0.48      0.47      1592\n",
      "weighted avg       0.51      0.50      0.50      1592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[110  91  69]\n",
      " [128 310 183]\n",
      " [130 202 369]]\n",
      "\n",
      "Random Forest:\n",
      "Accuracy: 0.5980\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.49      0.44       270\n",
      "           1       0.61      0.65      0.63       621\n",
      "           2       0.68      0.60      0.64       701\n",
      "\n",
      "    accuracy                           0.60      1592\n",
      "   macro avg       0.57      0.58      0.57      1592\n",
      "weighted avg       0.61      0.60      0.60      1592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[132  76  62]\n",
      " [ 88 401 132]\n",
      " [106 176 419]]\n",
      "\n",
      "XGBoost:\n",
      "Accuracy: 0.5710\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.51      0.42       270\n",
      "           1       0.60      0.57      0.59       621\n",
      "           2       0.68      0.59      0.63       701\n",
      "\n",
      "    accuracy                           0.57      1592\n",
      "   macro avg       0.55      0.56      0.55      1592\n",
      "weighted avg       0.60      0.57      0.58      1592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[139  76  55]\n",
      " [129 356 136]\n",
      " [130 157 414]]\n"
     ]
    }
   ],
   "source": [
    "print_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nPlotting confusion matrices...\")\n",
    "plot_confusion_matrices(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List selected features for Sub-Activity\n",
    "selected_features_sub = list(X_train.columns[selector.get_support()])\n",
    "print(\"Selected Features for Sub-Activity Detection:\")\n",
    "print(selected_features_sub)\n",
    "\n",
    "# List selected features for Knife Sharpness Detection\n",
    "selected_features_sharp = list(X_train.columns[selector.get_support()])\n",
    "print(\"\\nSelected Features for Knife Sharpness Detection:\")\n",
    "print(selected_features_sharp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Cross validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validating Models for Target: Main_activity\n",
      "Logistic Regression - Mean CV Accuracy: 0.7901, Std Dev: 0.0055\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     models \u001b[38;5;241m=\u001b[39m get_models(task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 12\u001b[0m     cv_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Mean CV Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv_scores\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Std Dev: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv_scores\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\MotionDataAnalysis\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32md:\\MotionDataAnalysis\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:712\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    710\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 712\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\MotionDataAnalysis\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32md:\\MotionDataAnalysis\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:423\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 423\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32md:\\MotionDataAnalysis\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MotionDataAnalysis\\venv\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32md:\\MotionDataAnalysis\\venv\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32md:\\MotionDataAnalysis\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MotionDataAnalysis\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32md:\\MotionDataAnalysis\\venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MotionDataAnalysis\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32md:\\MotionDataAnalysis\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MotionDataAnalysis\\venv\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32md:\\MotionDataAnalysis\\venv\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32md:\\MotionDataAnalysis\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MotionDataAnalysis\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32md:\\MotionDataAnalysis\\venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for target, (X_train, X_test, y_train, y_test) in normalized_data.items():\n",
    "    print(f\"\\nCross-Validating Models for Target: {target.capitalize()}\")\n",
    "    \n",
    "    if target == 'main_activity':\n",
    "        models = get_models(task_type='binary')\n",
    "    else:\n",
    "        models = get_models(task_type=\"multiclass\") \n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        print(f\"{model_name} - Mean CV Accuracy: {cv_scores.mean():.4f}, Std Dev: {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained models as pkl file to the models folder\n",
    "import pickle\n",
    "\n",
    "for target, target_models in trained_models.items():\n",
    "    for model_name, model in target_models.items():\n",
    "        filename = f\"../models/{target}_{model_name}_iter3.pkl\"\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(model, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
