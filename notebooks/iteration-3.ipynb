{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "sys.path.append('../')  # Go up one directory\n",
    "\n",
    "from src.data_splitting import split_data, get_split_shapes\n",
    "from src.model_training import get_models, train_models_for_task\n",
    "from src.model_evaluation import print_results, print_result_for_task, summarize_results, plot_confusion_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Import and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (7956, 135)\n",
      "\n",
      "main_activity splits:\n",
      "X_train shape: (6364, 132)\n",
      "X_test shape: (1592, 132)\n",
      "y_train shape: (6364,)\n",
      "y_test shape: (1592,)\n",
      "\n",
      "label splits:\n",
      "X_train shape: (6364, 132)\n",
      "X_test shape: (1592, 132)\n",
      "y_train shape: (6364,)\n",
      "y_test shape: (1592,)\n",
      "\n",
      "sharpness splits:\n",
      "X_train shape: (6364, 132)\n",
      "X_test shape: (1592, 132)\n",
      "y_train shape: (6364,)\n",
      "y_test shape: (1592,)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Read in the cleaned data\n",
    "cleaned_df = pd.read_csv('..\\\\data\\\\final_data\\\\cleaned_train_data.csv')\n",
    "print(f\"Dataset shape: {cleaned_df.shape}\")\n",
    "\n",
    "# Shuffle the data\n",
    "cleaned_df = cleaned_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "splits = split_data(cleaned_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(get_split_shapes(splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Feature engineering - Magnitude features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magnitude features added\n",
      "Number of magnitude features: 44\n",
      "(7956, 179)\n",
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "joints = ['L5', 'L3', 'T12', 'T8', 'Neck', 'Head',\n",
    "          'Right Shoulder', 'Right Upper Arm', 'Right Forearm', 'Right Hand',\n",
    "            'Left Shoulder', 'Left Upper Arm', 'Left Forearm', 'Left Hand', \n",
    "            'Right Upper Leg', 'Right Lower Leg', 'Right Foot', 'Right Toe',\n",
    "            'Left Upper Leg', 'Left Lower Leg', 'Left Foot', 'Left Toe', ]\n",
    "\n",
    "for joint in joints:\n",
    "\n",
    "    vel_cols = [f'{joint} x_Vel', f'{joint} y_Vel', f'{joint} z_Vel']\n",
    "    acc_cols = [f'{joint} x_Acc', f'{joint} y_Acc', f'{joint} z_Acc']\n",
    "\n",
    "\n",
    "    if all(col in cleaned_df.columns for col in vel_cols):\n",
    "        cleaned_df[f'{joint}_Vel_Magnitude'] = np.sqrt(\n",
    "            cleaned_df[vel_cols[0]]**2 + \n",
    "            cleaned_df[vel_cols[1]]**2 + \n",
    "            cleaned_df[vel_cols[2]]**2\n",
    "        )\n",
    "    \n",
    "    # Acceleration magnitude\n",
    "    if all(col in cleaned_df.columns for col in acc_cols):\n",
    "        cleaned_df[f'{joint}_Acc_Magnitude'] = np.sqrt(\n",
    "            cleaned_df[acc_cols[0]]**2 + \n",
    "            cleaned_df[acc_cols[1]]**2 + \n",
    "            cleaned_df[acc_cols[2]]**2\n",
    "        )\n",
    "\n",
    "print(\"Magnitude features added\")\n",
    "magnitude_cols = [col for col in cleaned_df.columns if 'Magnitude' in col]\n",
    "print(f\"Number of magnitude features: {len(magnitude_cols)}\")\n",
    "print(cleaned_df.shape)\n",
    "\n",
    "# Print the columns with misisng values > 0\n",
    "print(cleaned_df.columns[cleaned_df.isna().sum() > 0])\n",
    "\n",
    "# Fill the NaN values with the mean of the column\n",
    "# cleaned_df.fillna(cleaned_df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Feature engineering - Roll features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling features added\n",
      "Number of rolling features: 44\n"
     ]
    }
   ],
   "source": [
    "# Define window sizes for rolling calculations\n",
    "window_size = 5  # Single window size for simplicity\n",
    "\n",
    "# Calculate rolling mean for magnitude columns\n",
    "for col in magnitude_cols:\n",
    "    # Group by label to avoid mixing statistics across different activities\n",
    "    grouped = cleaned_df.groupby('Label')[col]\n",
    "    \n",
    "    # Calculate rolling mean only\n",
    "    cleaned_df[f'{col}_RollingMean_{window_size}'] = grouped.transform(\n",
    "        lambda x: x.rolling(window=window_size, min_periods=1).mean())\n",
    "\n",
    "print(\"Rolling features added\")\n",
    "rolling_cols = [col for col in cleaned_df.columns if 'Rolling' in col]\n",
    "print(f\"Number of rolling features: {len(rolling_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n",
      "(7956, 223)\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_df.columns[cleaned_df.isna().sum() > 0])\n",
    "print(cleaned_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(features_df, title, target_name, top_n=20):\n",
    "    \"\"\"Visualize feature importance\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='importance', y='feature', data=features_df.head(top_n))\n",
    "    plt.title(f'Top {top_n} Important Features for {title}\\nTarget: {target_name}')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 220\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for feature selection\n",
    "feature_cols = [col for col in cleaned_df.columns \n",
    "                if col not in ['Main_Activity', 'Label', 'Knife_Sharpness_Category']]\n",
    "X = cleaned_df[feature_cols]\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "print (f\"Number of features: {len(feature_cols)}\")\n",
    "\n",
    "# Dictionary to store selected features for each target\n",
    "selected_features = {}\n",
    "\n",
    "# Define targets and how many features to select for each\n",
    "targets = {\n",
    "    'main_activity': ('Main_Activity', 50),  # (target_column, n_features)\n",
    "    'label': ('Label', 70),\n",
    "    'sharpness': ('Knife_Sharpness_Category', 50)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Perform feature selection for each target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature selection for main_activity\n",
      "\n",
      "Number of selected features for main_activity: 27\n",
      "\n",
      "Top 10 selected features:\n",
      "['T12_Vel_Magnitude_RollingMean_5', 'Right Upper Arm_Acc_Magnitude_RollingMean_5', 'L3_Acc_Magnitude_RollingMean_5', 'L5 y_Vel', 'T12_Vel_Magnitude', 'Left Forearm_Acc_Magnitude_RollingMean_5', 'Right Lower Leg_Acc_Magnitude', 'Neck_Vel_Magnitude', 'T8_Acc_Magnitude_RollingMean_5', 'L3 z_Vel']\n",
      "\n",
      "Feature type breakdown:\n",
      "Original features: 9\n",
      "Magnitude features: 18\n",
      "Rolling features: 11\n",
      "\n",
      "Feature selection for label\n",
      "\n",
      "Number of selected features for label: 58\n",
      "\n",
      "Top 10 selected features:\n",
      "['Left Lower Leg_Acc_Magnitude_RollingMean_5', 'Left Lower Leg_Vel_Magnitude_RollingMean_5', 'T12_Vel_Magnitude_RollingMean_5', 'Right Upper Arm_Acc_Magnitude_RollingMean_5', 'L3_Acc_Magnitude_RollingMean_5', 'Left Toe_Vel_Magnitude_RollingMean_5', 'Right Lower Leg_Acc_Magnitude_RollingMean_5', 'Left Forearm_Acc_Magnitude_RollingMean_5', 'Right Upper Leg_Vel_Magnitude_RollingMean_5', 'Left Upper Leg_Acc_Magnitude']\n",
      "\n",
      "Feature type breakdown:\n",
      "Original features: 0\n",
      "Magnitude features: 58\n",
      "Rolling features: 44\n",
      "\n",
      "Feature selection for sharpness\n",
      "\n",
      "Number of selected features for sharpness: 32\n",
      "\n",
      "Top 10 selected features:\n",
      "['L5 x_Vel', 'L5 y_Vel', 'T12_Vel_Magnitude', 'Left Upper Leg_Acc_Magnitude', 'Right Upper Arm_Acc_Magnitude', 'Neck_Vel_Magnitude', 'Right Upper Leg_Acc_Magnitude', 'L3 z_Vel', 'Right Upper Leg z_Acc', 'Neck x_Vel']\n",
      "\n",
      "Feature type breakdown:\n",
      "Original features: 15\n",
      "Magnitude features: 17\n",
      "Rolling features: 1\n"
     ]
    }
   ],
   "source": [
    "for target_key, (target_col, n_features) in targets.items():\n",
    "    print(f\"\\nFeature selection for {target_key}\")\n",
    "    y = cleaned_df[target_col]\n",
    "    \n",
    "    # 1. Random Forest Importance\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    rf_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot Random Forest importance\n",
    "    # plot_feature_importance(rf_importance, f\"{target_key} (Random Forest)\")\n",
    "    \n",
    "    # 2. ANOVA F-scores\n",
    "    f_selector = SelectKBest(score_func=f_classif, k='all')\n",
    "    f_selector.fit(X, y)\n",
    "    f_scores = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': f_selector.scores_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot ANOVA F-scores\n",
    "    # plot_feature_importance(f_scores, f\"{target_key} (ANOVA F-scores)\")\n",
    "    \n",
    "    # Select features that are important in both methods\n",
    "    top_rf = set(rf_importance.head(n_features)['feature'])\n",
    "    top_f = set(f_scores.head(n_features)['feature'])\n",
    "    \n",
    "    # Get features that appear in both methods\n",
    "    common_features = list(top_rf.intersection(top_f))\n",
    "    selected_features[target_key] = common_features\n",
    "    \n",
    "    print(f\"\\nNumber of selected features for {target_key}: {len(common_features)}\")\n",
    "    print(\"\\nTop 10 selected features:\")\n",
    "    print(common_features[:10])\n",
    "    \n",
    "    # Analysis of selected feature types\n",
    "    magnitude_selected = len([f for f in common_features if 'Magnitude' in f])\n",
    "    rolling_selected = len([f for f in common_features if 'Rolling' in f])\n",
    "    original_selected = len([f for f in common_features \n",
    "                           if 'Magnitude' not in f and 'Rolling' not in f])\n",
    "    \n",
    "    print(f\"\\nFeature type breakdown:\")\n",
    "    print(f\"Original features: {original_selected}\")\n",
    "    print(f\"Magnitude features: {magnitude_selected}\")\n",
    "    print(f\"Rolling features: {rolling_selected}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating final datasets with selected features...\n",
      "\n",
      "main_activity dataset shape: (7956, 28)\n",
      "\n",
      "label dataset shape: (7956, 59)\n",
      "\n",
      "sharpness dataset shape: (7956, 33)\n",
      "\n",
      "Feature selection complete!\n",
      "\n",
      "Summary of selected features:\n",
      "\n",
      "main_activity:\n",
      "Number of features selected: 27\n",
      "\n",
      "label:\n",
      "Number of features selected: 58\n",
      "\n",
      "sharpness:\n",
      "Number of features selected: 32\n"
     ]
    }
   ],
   "source": [
    "# Create final datasets with selected features\n",
    "print(\"\\nCreating final datasets with selected features...\")\n",
    "selected_dfs = {}\n",
    "for target_key, features in selected_features.items():\n",
    "    if target_key == 'main_activity':\n",
    "        target_col = 'Main_Activity'\n",
    "    elif target_key == 'label':\n",
    "        target_col = 'Label'\n",
    "    else:  # sharpness\n",
    "        target_col = 'Knife_Sharpness_Category'\n",
    "    \n",
    "    # Create new dataframe with only selected features and target\n",
    "    selected_df = pd.concat([\n",
    "        cleaned_df[features],  # Selected features\n",
    "        cleaned_df[target_col]  # Target variable\n",
    "    ], axis=1)\n",
    "    \n",
    "    selected_dfs[target_key] = selected_df\n",
    "    print(f\"\\n{target_key} dataset shape: {selected_df.shape}\")\n",
    "\n",
    "# Save the selected features for reference\n",
    "feature_selection_summary = {\n",
    "    target: {\n",
    "        'selected_features': features,\n",
    "        'n_features': len(features)\n",
    "    }\n",
    "    for target, features in selected_features.items()\n",
    "}\n",
    "\n",
    "print(\"\\nFeature selection complete!\")\n",
    "print(\"\\nSummary of selected features:\")\n",
    "for target, info in feature_selection_summary.items():\n",
    "    print(f\"\\n{target}:\")\n",
    "    print(f\"Number of features selected: {info['n_features']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Split data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data splits after feature selection:\n",
      "\n",
      "main_activity splits:\n",
      "X_train shape: (6364, 27)\n",
      "X_test shape: (1592, 27)\n",
      "y_train shape: (6364,)\n",
      "y_test shape: (1592,)\n",
      "\n",
      "label splits:\n",
      "X_train shape: (6364, 58)\n",
      "X_test shape: (1592, 58)\n",
      "y_train shape: (6364,)\n",
      "y_test shape: (1592,)\n",
      "\n",
      "sharpness splits:\n",
      "X_train shape: (6364, 32)\n",
      "X_test shape: (1592, 32)\n",
      "y_train shape: (6364,)\n",
      "y_test shape: (1592,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "splits = {}\n",
    "for target_key, df in selected_dfs.items():\n",
    "    # Get features and target\n",
    "    X = df.drop(columns=['Main_Activity' if target_key == 'main_activity' \n",
    "                        else 'Label' if target_key == 'label' \n",
    "                        else 'Knife_Sharpness_Category'])\n",
    "    y = df['Main_Activity' if target_key == 'main_activity' \n",
    "           else 'Label' if target_key == 'label' \n",
    "           else 'Knife_Sharpness_Category']\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    splits[target_key] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"\\nData splits after feature selection:\")\n",
    "for target, (X_train, X_test, y_train, y_test) in splits.items():\n",
    "    print(f\"\\n{target} splits:\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Class balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: Main_activity\n",
      "Original class distribution: {0: np.int64(3350), 1: np.int64(3014)}\n",
      "Resampled class distribution: {1: np.int64(2970), 0: np.int64(2970)}\n",
      "Original shape: (6364, 27), Resampled shape: (5940, 27)\n",
      "\n",
      "Target: Label\n",
      "Original class distribution: {4: np.int64(3187), 0: np.int64(765), 5: np.int64(733), 2: np.int64(598), 8: np.int64(433), 3: np.int64(290), 1: np.int64(170), 7: np.int64(103), 6: np.int64(85)}\n",
      "Resampled class distribution: {7: np.int64(3187), 6: np.int64(3187), 1: np.int64(3187), 8: np.int64(3187), 2: np.int64(3187), 3: np.int64(3186), 0: np.int64(3186), 5: np.int64(3185), 4: np.int64(3183)}\n",
      "Original shape: (6364, 58), Resampled shape: (28675, 58)\n",
      "\n",
      "Target: Sharpness\n",
      "Original class distribution: {2: np.int64(2802), 1: np.int64(2481), 0: np.int64(1081)}\n",
      "Resampled class distribution: {0: np.int64(2773), 1: np.int64(2570), 2: np.int64(2563)}\n",
      "Original shape: (6364, 32), Resampled shape: (7906, 32)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Initialize SMOTETomek\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "\n",
    "# Loop through each target variable to apply SMOTE-Tomek\n",
    "balanced_data = {}\n",
    "for target in ['main_activity', 'label', 'sharpness']:\n",
    "\n",
    "    # Get the data splits\n",
    "    X_train, X_test, y_train, y_test = splits[target]\n",
    "    print(f\"\\nTarget: {target.capitalize()}\")\n",
    "    print(f\"Original class distribution: {dict(pd.Series(y_train).value_counts())}\")\n",
    "\n",
    "    # Apply SMOTE-Tomek\n",
    "    X_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "    balanced_data[target] = (X_resampled, X_test, y_resampled, y_test)\n",
    "\n",
    "    print(f\"Resampled class distribution: {dict(pd.Series(y_resampled).value_counts())}\")\n",
    "    print(f\"Original shape: {X_train.shape}, Resampled shape: {X_resampled.shape}\")\n",
    "\n",
    "\n",
    "    # print(f\"\\nTarget: {target.capitalize()}\")\n",
    "    # print(f\"Original class distribution: {dict(pd.Series(y_train).value_counts())}\")\n",
    "    # print(f\"Resampled class distribution: {dict(pd.Series(y_resampled).value_counts())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization complete for Main_activity\n",
      "Training data shape: (5940, 27)\n",
      "Test data shape: (1592, 27)\n",
      "Normalization complete for Label\n",
      "Training data shape: (28675, 58)\n",
      "Test data shape: (1592, 58)\n",
      "Normalization complete for Sharpness\n",
      "Training data shape: (7906, 32)\n",
      "Test data shape: (1592, 32)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "normalized_data = {}\n",
    "\n",
    "# Loop through balanced data and normalize\n",
    "for target, (X_resampled, X_test, y_resampled, y_test) in balanced_data.items():\n",
    "\n",
    "    # Fit on training data, transform both training and test\n",
    "    X_normalized = scaler.fit_transform(X_resampled)\n",
    "    X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "    normalized_data[target] = (X_normalized, X_test_normalized, y_resampled, y_test)\n",
    "    \n",
    "    print(f\"Normalization complete for {target.capitalize()}\")\n",
    "    print(f\"Training data shape: {X_normalized.shape}\")\n",
    "    print(f\"Test data shape: {X_test_normalized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "main_activity splits:\n",
      "X_train shape: (5940, 27)\n",
      "X_test shape: (1592, 27)\n",
      "y_train shape: (5940,)\n",
      "y_test shape: (1592,)\n",
      "\n",
      "label splits:\n",
      "X_train shape: (28675, 58)\n",
      "X_test shape: (1592, 58)\n",
      "y_train shape: (28675,)\n",
      "y_test shape: (1592,)\n",
      "\n",
      "sharpness splits:\n",
      "X_train shape: (7906, 32)\n",
      "X_test shape: (1592, 32)\n",
      "y_train shape: (7906,)\n",
      "y_test shape: (1592,)\n"
     ]
    }
   ],
   "source": [
    "# print the targets from normalized data\n",
    "for target, (X_train, X_test, y_train, y_test) in normalized_data.items():\n",
    "    print(f\"\\n{target} splits:\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "\n",
      "Training Models for Target: Main_activity\n",
      "\n",
      "Training Logistic Regression...\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Training XGBoost...\n",
      "----------------------------------------\n",
      "\n",
      "Training Models for Target: Label\n",
      "\n",
      "Training Decision Tree...\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Training XGBoost...\n",
      "----------------------------------------\n",
      "\n",
      "Training Models for Target: Sharpness\n",
      "\n",
      "Training Decision Tree...\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Training XGBoost...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Dictionary to store results for each target\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "# Train and evaluate models\n",
    "for target, (X_train, X_test, y_train, y_test) in normalized_data.items():\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"\\nTraining Models for Target: {target.capitalize()}\")\n",
    "   \n",
    "    if target == 'main_activity':\n",
    "        models = get_models(task_type='binary')\n",
    "    else:\n",
    "        models = get_models(task_type=\"multiclass\")  \n",
    "\n",
    "    target_results = {}\n",
    "    target_trained_models = {}\n",
    "    for model_name, model in models.items():\n",
    "        \n",
    "        # Train model\n",
    "        print(f\"\\nTraining {model_name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        target_trained_models[model_name] = model\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        target_results[model_name] = {\n",
    "            'model': model,\n",
    "            'accuracy': accuracy,\n",
    "            'report': class_report,\n",
    "            'confusion_matrix': conf_matrix,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "    \n",
    "    results[target] = target_results\n",
    "    trained_models[target] = target_trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for main_activity:\n",
      "--------------------------------------------------\n",
      "\n",
      "Logistic Regression:\n",
      "Accuracy: 0.7519\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.76      0.76       838\n",
      "           1       0.74      0.74      0.74       754\n",
      "\n",
      "    accuracy                           0.75      1592\n",
      "   macro avg       0.75      0.75      0.75      1592\n",
      "weighted avg       0.75      0.75      0.75      1592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[639 199]\n",
      " [196 558]]\n",
      "\n",
      "Random Forest:\n",
      "Accuracy: 0.7814\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.79       838\n",
      "           1       0.77      0.77      0.77       754\n",
      "\n",
      "    accuracy                           0.78      1592\n",
      "   macro avg       0.78      0.78      0.78      1592\n",
      "weighted avg       0.78      0.78      0.78      1592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[660 178]\n",
      " [170 584]]\n",
      "\n",
      "XGBoost:\n",
      "Accuracy: 0.7927\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.80       838\n",
      "           1       0.78      0.78      0.78       754\n",
      "\n",
      "    accuracy                           0.79      1592\n",
      "   macro avg       0.79      0.79      0.79      1592\n",
      "weighted avg       0.79      0.79      0.79      1592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[677 161]\n",
      " [169 585]]\n",
      "\n",
      "Results for label:\n",
      "--------------------------------------------------\n",
      "\n",
      "Decision Tree:\n",
      "Accuracy: 0.6683\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.87       191\n",
      "           1       0.83      0.79      0.81        43\n",
      "           2       0.75      0.82      0.78       150\n",
      "           3       0.35      0.49      0.41        72\n",
      "           4       0.85      0.67      0.75       797\n",
      "           5       0.32      0.43      0.37       184\n",
      "           6       0.18      0.29      0.22        21\n",
      "           7       0.32      0.65      0.43        26\n",
      "           8       0.54      0.65      0.59       108\n",
      "\n",
      "    accuracy                           0.67      1592\n",
      "   macro avg       0.56      0.63      0.58      1592\n",
      "weighted avg       0.72      0.67      0.68      1592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[166   1  15   0   3   4   0   0   2]\n",
      " [  1  34   0   4   4   0   0   0   0]\n",
      " [ 11   0 123   1   7   6   0   0   2]\n",
      " [  0   1   1  35   8   5   3   5  14]\n",
      " [ 10   4  19  25 533 143  15  21  27]\n",
      " [  4   1   6  10  56  80   8   5  14]\n",
      " [  0   0   0   4   4   4   6   3   0]\n",
      " [  0   0   0   4   3   0   1  17   1]\n",
      " [  0   0   0  16  12   8   0   2  70]]\n",
      "\n",
      "Random Forest:\n",
      "Accuracy: 0.7971\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94       191\n",
      "           1       0.91      1.00      0.96        43\n",
      "           2       0.83      0.91      0.87       150\n",
      "           3       0.61      0.62      0.62        72\n",
      "           4       0.89      0.81      0.85       797\n",
      "           5       0.48      0.54      0.51       184\n",
      "           6       0.43      0.62      0.51        21\n",
      "           7       0.53      0.81      0.64        26\n",
      "           8       0.73      0.81      0.77       108\n",
      "\n",
      "    accuracy                           0.80      1592\n",
      "   macro avg       0.71      0.79      0.74      1592\n",
      "weighted avg       0.81      0.80      0.80      1592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[179   0   8   1   2   1   0   0   0]\n",
      " [  0  43   0   0   0   0   0   0   0]\n",
      " [  5   0 137   0   4   4   0   0   0]\n",
      " [  2   1   0  45   5   2   2   2  13]\n",
      " [  1   2  14   8 643  94   9  11  15]\n",
      " [  1   0   6   7  57 100   5   4   4]\n",
      " [  0   1   0   1   1   2  13   2   1]\n",
      " [  0   0   0   2   2   0   1  21   0]\n",
      " [  0   0   0  10   5   5   0   0  88]]\n",
      "\n",
      "XGBoost:\n",
      "Accuracy: 0.8160\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       191\n",
      "           1       0.89      0.98      0.93        43\n",
      "           2       0.87      0.92      0.90       150\n",
      "           3       0.69      0.67      0.68        72\n",
      "           4       0.92      0.83      0.87       797\n",
      "           5       0.52      0.63      0.57       184\n",
      "           6       0.34      0.52      0.42        21\n",
      "           7       0.50      0.73      0.59        26\n",
      "           8       0.72      0.81      0.77       108\n",
      "\n",
      "    accuracy                           0.82      1592\n",
      "   macro avg       0.71      0.78      0.74      1592\n",
      "weighted avg       0.84      0.82      0.82      1592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[179   1   7   1   0   2   0   0   1]\n",
      " [  1  42   0   0   0   0   0   0   0]\n",
      " [  5   0 138   0   1   5   0   0   1]\n",
      " [  1   1   0  48   6   1   2   1  12]\n",
      " [  0   2   8   4 658  93   9  10  13]\n",
      " [  0   0   5   4  42 116   7   4   6]\n",
      " [  0   1   0   1   1   3  11   3   1]\n",
      " [  0   0   0   1   3   1   2  19   0]\n",
      " [  0   0   0  11   3   4   1   1  88]]\n",
      "\n",
      "Results for sharpness:\n",
      "--------------------------------------------------\n",
      "\n",
      "Decision Tree:\n",
      "Accuracy: 0.4956\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.41      0.34       270\n",
      "           1       0.51      0.50      0.51       621\n",
      "           2       0.59      0.53      0.56       701\n",
      "\n",
      "    accuracy                           0.50      1592\n",
      "   macro avg       0.47      0.48      0.47      1592\n",
      "weighted avg       0.51      0.50      0.50      1592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[110  91  69]\n",
      " [128 310 183]\n",
      " [130 202 369]]\n",
      "\n",
      "Random Forest:\n",
      "Accuracy: 0.5980\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.49      0.44       270\n",
      "           1       0.61      0.65      0.63       621\n",
      "           2       0.68      0.60      0.64       701\n",
      "\n",
      "    accuracy                           0.60      1592\n",
      "   macro avg       0.57      0.58      0.57      1592\n",
      "weighted avg       0.61      0.60      0.60      1592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[132  76  62]\n",
      " [ 88 401 132]\n",
      " [106 176 419]]\n",
      "\n",
      "XGBoost:\n",
      "Accuracy: 0.5710\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.51      0.42       270\n",
      "           1       0.60      0.57      0.59       621\n",
      "           2       0.68      0.59      0.63       701\n",
      "\n",
      "    accuracy                           0.57      1592\n",
      "   macro avg       0.55      0.56      0.55      1592\n",
      "weighted avg       0.60      0.57      0.58      1592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[139  76  55]\n",
      " [129 356 136]\n",
      " [130 157 414]]\n"
     ]
    }
   ],
   "source": [
    "print_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nPlotting confusion matrices...\")\n",
    "plot_confusion_matrices(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Cross validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validating Models for Target: Main_activity\n",
      "Logistic Regression - Mean CV Accuracy: 0.7901, Std Dev: 0.0055\n",
      "Random Forest - Mean CV Accuracy: 0.8350, Std Dev: 0.0083\n",
      "XGBoost - Mean CV Accuracy: 0.8387, Std Dev: 0.0087\n",
      "\n",
      "Cross-Validating Models for Target: Label\n",
      "Decision Tree - Mean CV Accuracy: 0.8966, Std Dev: 0.0108\n",
      "Random Forest - Mean CV Accuracy: 0.9564, Std Dev: 0.0067\n",
      "XGBoost - Mean CV Accuracy: 0.9644, Std Dev: 0.0085\n",
      "\n",
      "Cross-Validating Models for Target: Sharpness\n",
      "Decision Tree - Mean CV Accuracy: 0.5806, Std Dev: 0.0429\n",
      "Random Forest - Mean CV Accuracy: 0.6872, Std Dev: 0.0323\n",
      "XGBoost - Mean CV Accuracy: 0.6662, Std Dev: 0.0380\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for target, (X_train, X_test, y_train, y_test) in normalized_data.items():\n",
    "    print(f\"\\nCross-Validating Models for Target: {target.capitalize()}\")\n",
    "    \n",
    "    if target == 'main_activity':\n",
    "        models = get_models(task_type='binary')\n",
    "    else:\n",
    "        models = get_models(task_type=\"multiclass\") \n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        print(f\"{model_name} - Mean CV Accuracy: {cv_scores.mean():.4f}, Std Dev: {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained models as pkl file to the models folder\n",
    "import pickle\n",
    "\n",
    "for target, target_models in trained_models.items():\n",
    "    for model_name, model in target_models.items():\n",
    "        filename = f\"../models/{target}_{model_name}_iter3.pkl\"\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(model, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
